{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from detection_utils import random_label\n",
    "from detection_evaluate import single_metric_fpr_tpr, combined_metric_fpr_tpr\n",
    "from detection_baseline import targeted_vals, untargeted_vals\n",
    "\n",
    "# from utils import transform\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import torch.utils.data as data\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from timm.data.constants import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cpu\n",
      "=> load checkpoint found at /Users/albertwen/Downloads/mae_data/resnet50.pth\n"
     ]
    }
   ],
   "source": [
    "model = torchvision.models.resnet50(pretrained=True)\n",
    "\n",
    "# Path to the folder where the pretrained models are saved\n",
    "CHECKPOINT_PATH = \"/Users/albertwen/Downloads/mae_data/resnet50.pth\"\n",
    "\n",
    "# Setting the seed\n",
    "# pl.seed_everything(42)\n",
    "\n",
    "# Ensure that all operations are deterministic on GPU (if used) for reproducibility\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Fetching the device that will be used throughout this notebook\n",
    "device = torch.device(\"cpu\") if not torch.cuda.is_available() else torch.device(\"cuda:0\")\n",
    "print(\"Using device\", device)\n",
    "\n",
    "pretrained_model = torchvision.models.resnet50(pretrained=True)\n",
    "pretrained_model = pretrained_model.to(device)\n",
    "\n",
    "if CHECKPOINT_PATH:\n",
    "    if os.path.isfile(CHECKPOINT_PATH):\n",
    "        checkpoint = torch.load(CHECKPOINT_PATH) \n",
    "        # model.load_state_dict(checkpoint['state_dict'])  #['state_dict']\n",
    "        pretrained_model.load_state_dict(checkpoint)  #['state_dict']\n",
    "        print(\"=> load checkpoint found at {}\".format(CHECKPOINT_PATH))\n",
    "        # print(\"=> loaded checkpoint '{}' (epoch {})\"\n",
    "        #       .format(args.resume, checkpoint['epoch']))\n",
    "    else:\n",
    "        print(\"=> no checkpoint found at '{}'\".format(CHECKPOINT_PATH))\n",
    "# No gradients needed for the network\n",
    "pretrained_model.eval()\n",
    "for p in pretrained_model.parameters():\n",
    "    p.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr = 0.1 # can be [0.1, 0.2]\n",
    "untargeted_step_threshold = 1000\n",
    "criterions = {0.1: (0.190, 35, untargeted_step_threshold), 0.2: (1.77, 22, untargeted_step_threshold)} # pre-defined in paper's code\n",
    "dataset = \"imagenet\"\n",
    "attacks = [\"fgsm_4\", \"fgsm_8\", \"pgd_8\"]\n",
    "real_d = \"/Users/albertwen/Downloads/mae_data/ImageNet-Data/val\"\n",
    "adv_d = \"/Users/albertwen/Downloads/mae_data/ImageNet-Data/attack/val/\"\n",
    "\n",
    "noise_radius = 0.1\n",
    "targeted_lr = 0.005\n",
    "targeted_radius = 0.03\n",
    "untargeted_radius = 0.03\n",
    "untargeted_lr = 0.1\n",
    "\n",
    "det_opt = \"untargeted\" # can be [\"targeted\", \"untargeted\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m single_metric_fpr_tpr(fpr, \n\u001b[1;32m      2\u001b[0m                       criterions, \n\u001b[1;32m      3\u001b[0m                       pretrained_model, \n\u001b[1;32m      4\u001b[0m                       dataset, \n\u001b[1;32m      5\u001b[0m                       attacks, \n\u001b[1;32m      6\u001b[0m                       real_d, \n\u001b[1;32m      7\u001b[0m                       adv_d, \n\u001b[1;32m      8\u001b[0m                       noise_radius, \n\u001b[1;32m      9\u001b[0m                       targeted_lr, \n\u001b[1;32m     10\u001b[0m                       targeted_radius, \n\u001b[1;32m     11\u001b[0m                       untargeted_lr, \n\u001b[1;32m     12\u001b[0m                       untargeted_radius, \n\u001b[1;32m     13\u001b[0m                       opt\u001b[39m=\u001b[39;49mdet_opt)\n",
      "File \u001b[0;32m~/GitHub/MAE-Reverse-Adversarial/detection_evaluate.py:32\u001b[0m, in \u001b[0;36msingle_metric_fpr_tpr\u001b[0;34m(fpr, criterions, model, dataset, attacks, real_dir, adv_dir, n_radius, targeted_lr, t_radius, untargeted_lr, u_radius, opt)\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mnumber of steps for targeted attacks on real images\u001b[39m\u001b[39m'\u001b[39m, target)\n\u001b[1;32m     31\u001b[0m \u001b[39melif\u001b[39;00m opt \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39muntargeted\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m---> 32\u001b[0m     target \u001b[39m=\u001b[39m untargeted_vals(model, dataset, \u001b[39m\"\u001b[39;49m\u001b[39mreal\u001b[39;49m\u001b[39m\"\u001b[39;49m, real_dir, adv_dir, untargeted_lr, u_radius)\n\u001b[1;32m     33\u001b[0m     threshold \u001b[39m=\u001b[39m criterions[fpr][\u001b[39m2\u001b[39m]\n\u001b[1;32m     34\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mnumber of steps for untargeted attacks for real images\u001b[39m\u001b[39m\"\u001b[39m, target)\n",
      "File \u001b[0;32m~/GitHub/MAE-Reverse-Adversarial/detection_baseline.py:219\u001b[0m, in \u001b[0;36muntargeted_vals\u001b[0;34m(model, dataset, attack, real_dir, adv_dir, untargeted_lr, u_radius)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[39mif\u001b[39;00m attack \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mreal\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    218\u001b[0m     image_dir \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(real_dir, \u001b[39m'\u001b[39m\u001b[39mval\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m--> 219\u001b[0m     \u001b[39massert\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(image_dir)\n\u001b[1;32m    220\u001b[0m     val_dataset \u001b[39m=\u001b[39m torchvision\u001b[39m.\u001b[39mdatasets\u001b[39m.\u001b[39mImageFolder(root\u001b[39m=\u001b[39mimage_dir, transform\u001b[39m=\u001b[39mplain_transforms)\n\u001b[1;32m    221\u001b[0m     val_dataset_loader \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mDataLoader(val_dataset, batch_size\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, shuffle\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, drop_last\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, num_workers\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "single_metric_fpr_tpr(fpr, \n",
    "                      criterions, \n",
    "                      pretrained_model, \n",
    "                      dataset, \n",
    "                      attacks, \n",
    "                      real_d, \n",
    "                      adv_d, \n",
    "                      noise_radius, \n",
    "                      targeted_lr, \n",
    "                      targeted_radius, \n",
    "                      untargeted_lr, \n",
    "                      untargeted_radius, \n",
    "                      opt=det_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f9992779686633b1a0c917c3f6317d760e57d78a77247aa4b25ad344de6939a7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
