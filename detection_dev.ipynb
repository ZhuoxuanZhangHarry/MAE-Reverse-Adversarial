{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from detection_utils import random_label\n",
    "\n",
    "# from utils import transform\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import torch.utils.data as data\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from timm.data.constants import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cpu\n",
      "=> load chechpoint found at /Users/albertwen/Downloads/mae_data/resnet50.pth\n"
     ]
    }
   ],
   "source": [
    "model = torchvision.models.resnet50(pretrained=True)\n",
    "\n",
    "# Path to the folder where the pretrained models are saved\n",
    "CHECKPOINT_PATH = \"/Users/albertwen/Downloads/mae_data/resnet50.pth\"\n",
    "\n",
    "# Setting the seed\n",
    "# pl.seed_everything(42)\n",
    "\n",
    "# Ensure that all operations are deterministic on GPU (if used) for reproducibility\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Fetching the device that will be used throughout this notebook\n",
    "device = torch.device(\"cpu\") if not torch.cuda.is_available() else torch.device(\"cuda:0\")\n",
    "print(\"Using device\", device)\n",
    "\n",
    "pretrained_model = torchvision.models.resnet50(pretrained=True)\n",
    "pretrained_model = pretrained_model.to(device)\n",
    "\n",
    "if CHECKPOINT_PATH:\n",
    "    if os.path.isfile(CHECKPOINT_PATH):\n",
    "        checkpoint = torch.load(CHECKPOINT_PATH) \n",
    "        # model.load_state_dict(checkpoint['state_dict'])  #['state_dict']\n",
    "        pretrained_model.load_state_dict(checkpoint)  #['state_dict']\n",
    "        print(\"=> load chechpoint found at {}\".format(CHECKPOINT_PATH))\n",
    "        # print(\"=> loaded checkpoint '{}' (epoch {})\"\n",
    "        #       .format(args.resume, checkpoint['epoch']))\n",
    "    else:\n",
    "        print(\"=> no checkpoint found at '{}'\".format(CHECKPOINT_PATH))\n",
    "# No gradients needed for the network\n",
    "pretrained_model.eval()\n",
    "for p in pretrained_model.parameters():\n",
    "    p.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "plain_transforms = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize(256), transforms.CenterCrop(224),\n",
    "    transforms.Normalize(IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagenet_path = \"/Users/albertwen/Downloads/mae_data/ImageNet-Data/\"\n",
    "val_dataset = torchvision.datasets.ImageFolder(root=os.path.join(imagenet_path, 'val'), transform=plain_transforms)\n",
    "val_dataset_loader = data.DataLoader(val_dataset, batch_size=1, shuffle=False, drop_last=False, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preds = []\n",
    "# reals = []\n",
    "\n",
    "# for images, labels in (val_dataset_loader):\n",
    "#     if labels.data != 0:\n",
    "#         break\n",
    "#     preds.append(model(images).data.max(1, keepdim=True)[1][0])\n",
    "#     reals.append(labels.data)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pretrained_model(x_var.clone()).data.max(1, keepdim=True)[1][0].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagenet_path = \"/Users/albertwen/Downloads/mae_data/ImageNet-Data/\"\n",
    "val_dataset = torchvision.datasets.ImageFolder(root=os.path.join(imagenet_path, \"test_val3\"), transform=plain_transforms)\n",
    "val_dataset_loader = data.DataLoader(val_dataset, batch_size=1, shuffle=False, drop_last=False, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "for images, label in val_dataset_loader:\n",
    "    labels.append(label.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([0]),\n",
       " tensor([0]),\n",
       " tensor([0]),\n",
       " tensor([0]),\n",
       " tensor([0]),\n",
       " tensor([0]),\n",
       " tensor([0]),\n",
       " tensor([0]),\n",
       " tensor([0]),\n",
       " tensor([0]),\n",
       " tensor([0]),\n",
       " tensor([0]),\n",
       " tensor([0]),\n",
       " tensor([0]),\n",
       " tensor([0]),\n",
       " tensor([0]),\n",
       " tensor([0]),\n",
       " tensor([0]),\n",
       " tensor([0]),\n",
       " tensor([0]),\n",
       " tensor([0]),\n",
       " tensor([0]),\n",
       " tensor([0]),\n",
       " tensor([0]),\n",
       " tensor([0]),\n",
       " tensor([0]),\n",
       " tensor([0]),\n",
       " tensor([0]),\n",
       " tensor([0]),\n",
       " tensor([0]),\n",
       " tensor([0]),\n",
       " tensor([0]),\n",
       " tensor([0]),\n",
       " tensor([0]),\n",
       " tensor([0]),\n",
       " tensor([0]),\n",
       " tensor([0]),\n",
       " tensor([0]),\n",
       " tensor([0]),\n",
       " tensor([0]),\n",
       " tensor([0]),\n",
       " tensor([0]),\n",
       " tensor([0]),\n",
       " tensor([0]),\n",
       " tensor([0]),\n",
       " tensor([0]),\n",
       " tensor([0]),\n",
       " tensor([0]),\n",
       " tensor([0]),\n",
       " tensor([0]),\n",
       " tensor([0]),\n",
       " tensor([0]),\n",
       " tensor([0]),\n",
       " tensor([0]),\n",
       " tensor([0]),\n",
       " tensor([0]),\n",
       " tensor([0]),\n",
       " tensor([0]),\n",
       " tensor([0]),\n",
       " tensor([0]),\n",
       " tensor([0]),\n",
       " tensor([0]),\n",
       " tensor([0]),\n",
       " tensor([0]),\n",
       " tensor([0]),\n",
       " tensor([0]),\n",
       " tensor([0]),\n",
       " tensor([0]),\n",
       " tensor([0]),\n",
       " tensor([0]),\n",
       " tensor([0]),\n",
       " tensor([0]),\n",
       " tensor([0]),\n",
       " tensor([0]),\n",
       " tensor([0]),\n",
       " tensor([0]),\n",
       " tensor([0]),\n",
       " tensor([0]),\n",
       " tensor([0]),\n",
       " tensor([0]),\n",
       " tensor([0]),\n",
       " tensor([0]),\n",
       " tensor([0]),\n",
       " tensor([0]),\n",
       " tensor([0]),\n",
       " tensor([0]),\n",
       " tensor([0]),\n",
       " tensor([0]),\n",
       " tensor([0]),\n",
       " tensor([0]),\n",
       " tensor([0]),\n",
       " tensor([0]),\n",
       " tensor([0]),\n",
       " tensor([0]),\n",
       " tensor([0]),\n",
       " tensor([0]),\n",
       " tensor([0]),\n",
       " tensor([0]),\n",
       " tensor([0]),\n",
       " tensor([0])]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def untargeted_detection(model, img, true_label, dataset, lr, u_radius, cap=1000, margin=20, use_margin=False):\n",
    "    model.eval()\n",
    "    # x_var = torch.autograd.Variable(img.clone().cuda(), requires_grad=True)\n",
    "    x_var = torch.autograd.Variable(img.clone(), requires_grad=True)\n",
    "    # true_label = model(x_var.clone()).data.max(1, keepdim=True)[1][0].item()\n",
    "    optimizer_s = optim.SGD([x_var], lr=lr)\n",
    "    counter = 0\n",
    "    while model(x_var.clone()).data.max(1, keepdim=True)[1][0].item() == true_label:\n",
    "        optimizer_s.zero_grad()\n",
    "        output = model(x_var)\n",
    "        if use_margin:\n",
    "            _, top2_1 = output.data.cpu().topk(2)\n",
    "            argmaxl1 = top2_1[0][0]\n",
    "            if argmaxl1 == true_label:\n",
    "                argmaxl1 = top2_1[0][1]\n",
    "            loss = (output[0][true_label] - output[0][argmaxl1] + margin).clamp(min=0)\n",
    "        else: \n",
    "            # loss = -F.cross_entropy(output, torch.LongTensor([true_label]).cuda())\n",
    "            loss = -F.cross_entropy(output, torch.LongTensor([true_label]))\n",
    "        loss.backward()\n",
    "\n",
    "        x_var.data = torch.clamp(x_var - lr * x_var.grad.data, min=0, max=1)\n",
    "        x_var.data = torch.clamp(x_var - img, min=-u_radius, max=u_radius) + img\n",
    "        counter += 1\n",
    "        if counter >= cap:\n",
    "            break\n",
    "    return counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def untargeted_vals(model, dataset, attack, real_dir, adv_dir, untargeted_lr, u_radius):\n",
    "    vals = np.zeros(0)\n",
    "    if attack == \"real\":\n",
    "\n",
    "        image_dir = os.path.join(real_dir, 'val')\n",
    "        assert os.path.exists(image_dir)\n",
    "        val_dataset = torchvision.datasets.ImageFolder(root=image_dir, transform=plain_transforms)\n",
    "        val_dataset_loader = data.DataLoader(val_dataset, batch_size=1, shuffle=False, drop_last=False, num_workers=2)\n",
    "        for img, label in (val_dataset_loader):\n",
    "\n",
    "        # for i in range(lowind, upind):\n",
    "            # image_dir = os.path.join(real_dir, str(i) + '_img.pt')\n",
    "            # assert os.path.exists(image_dir)\n",
    "            # view_data = torch.load(image_dir)\n",
    "\n",
    "            model.eval()\n",
    "            val = untargeted_detection(model, img, label, dataset, untargeted_lr, u_radius)\n",
    "            vals = np.concatenate((vals, [val]))\n",
    "    else: \n",
    "        cout = 0\n",
    "        image_dir = os.path.join(os.path.join(adv_dir, \"val\"), attack)\n",
    "        assert os.path.exists(image_dir)\n",
    "        atk_dataset = torchvision.datasets.ImageFolder(root=image_dir, transform=plain_transforms)\n",
    "        atk_dataset_loader = data.DataLoader(atk_dataset, batch_size=1, shuffle=False, drop_last=False, num_workers=2)\n",
    "        for img, label in (atk_dataset_loader):\n",
    "\n",
    "        # for i in range(lowind, upind):\n",
    "            # image_dir = os.path.join(os.path.join(adv_dir, attack), str(i) + title + '.pt')\n",
    "            # assert os.path.exists(image_dir)\n",
    "            # adv = torch.load(image_dir)\n",
    "            # real_label = torch.load(os.path.join(real_dir, str(i) + \"_label.pt\")) # TODO: edit\n",
    "            model.eval()\n",
    "            # predicted_label = model(transform(adv.clone(), dataset=dataset)).data.max(1, keepdim=True)[1][0]\n",
    "            predicted_label = model(img).data.max(1, keepdim=True)[1][0]\n",
    "            real_label = label\n",
    "            if real_label == predicted_label:\n",
    "                cout -= 1\n",
    "                continue\n",
    "            val = untargeted_detection(model, img, dataset, untargeted_lr, u_radius)\n",
    "            vals = np.concatenate((vals, [val]))\n",
    "        print('this is the number of successes in untargeted detection', cout)\n",
    "    return vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def targeted_detection(model, img, true_label, dataset, lr, t_radius, cap=200, margin=20, use_margin=False):\n",
    "    model.eval()\n",
    "    # x_var = torch.autograd.Variable(img.clone().cuda(), requires_grad=True\n",
    "    x_var = torch.autograd.Variable(img.clone(), requires_grad=True)\n",
    "    # true_label = model(transform(x_var.clone(), dataset=dataset)).data.max(1, keepdim=True)[1][0].item()\n",
    "    # change this function to pass in the true label\n",
    "\n",
    "    optimizer_s = optim.SGD([x_var], lr=lr)\n",
    "    # target_l = torch.LongTensor([random_label(true_label, dataset=dataset)]).cuda()\n",
    "    target_l = torch.LongTensor([random_label(true_label, dataset=dataset)])\n",
    "    counter = 0\n",
    "\n",
    "    # while model(transform(x_var.clone(), dataset=dataset)).data.max(1, keepdim=True)[1][0].item() == true_label: \n",
    "    while model(x_var.clone()).data.max(1, keepdim=True)[1][0].item() == true_label:\n",
    "        optimizer_s.zero_grad()\n",
    "        # output = model(transform(x_var, dataset=dataset))\n",
    "        output = model(x_var)\n",
    "        if use_margin:\n",
    "            target_l = target_l[0].item()\n",
    "            _, top2_1 = output.data.cpu().topk(2)\n",
    "            argmaxl1 = top2_1[0][0]\n",
    "            if argmaxl1 == target_l:\n",
    "                argmaxl1 = top2_1[0][1]\n",
    "            loss = (output[0][argmaxl1] - output[0][target_l] + margin).clamp(min=0)\n",
    "        else:\n",
    "            loss = F.cross_entropy(output, target_l)\n",
    "        loss.backward()\n",
    "\n",
    "        x_var.data = torch.clamp(x_var - lr * x_var.grad.data, min=0, max=1)\n",
    "        x_var.data = torch.clamp(x_var - img, min=-t_radius, max=t_radius) + img\n",
    "        counter += 1\n",
    "        if counter >= cap:\n",
    "            break\n",
    "    return counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def targeted_vals(model, dataset, attack, real_dir, adv_dir, targeted_lr, t_radius):\n",
    "    vals = np.zeros(0)\n",
    "    if attack == \"real\":\n",
    "        # for i in range(lowind, upind):\n",
    "\n",
    "        # imagenet_path = \"/Users/albertwen/Downloads/mae_data/ImageNet-Data/\"\n",
    "        image_dir = real_dir\n",
    "        assert os.path.exists(image_dir)\n",
    "        val_dataset = torchvision.datasets.ImageFolder(root=image_dir, transform=plain_transforms)\n",
    "        val_dataset_loader = data.DataLoader(val_dataset, batch_size=1, shuffle=False, drop_last=False, num_workers=2)\n",
    "\n",
    "        iters = 0\n",
    "        for img, label in tqdm(val_dataset_loader, desc=\"Validating...\"):\n",
    "            # image_dir = os.path.join(real_dir, str(i) + \"_img.pt\") # TODO: edit\n",
    "            # assert os.path.exists(image_dir)\n",
    "        \n",
    "            model.eval()\n",
    "            # print(\"targeted vals img\", img)\n",
    "            # print(\"targeted vals label\", label)\n",
    "            predicted_label = model(img.clone()).data.max(1, keepdim=True)[1][0]\n",
    "            if label != predicted_label:\n",
    "                continue\n",
    "\n",
    "            val = targeted_detection(model, img, label, dataset, targeted_lr, t_radius)\n",
    "            vals = np.concatenate((vals, [val]))\n",
    "            iters += 1\n",
    "            if iters > 5:\n",
    "                break\n",
    "\n",
    "    else: \n",
    "        cout = 0 # TODO: edit if statement logic\n",
    "\n",
    "        iters = 0\n",
    "        image_dir = adv_dir\n",
    "        assert os.path.exists(image_dir)\n",
    "        atk_dataset = torchvision.datasets.ImageFolder(root=image_dir, transform=plain_transforms)\n",
    "        atk_dataset_loader = data.DataLoader(atk_dataset, batch_size=1, shuffle=False, drop_last=False, num_workers=2)\n",
    "\n",
    "        for img, label in tqdm(atk_dataset_loader, desc=\"Validating...\"):\n",
    "        # for i in range(lowind, upind):\n",
    "            # image_dir = os.path.join(os.path.join(adv_dir, attack), str(i) + title + '.pt')\n",
    "            # assert os.path.exists(image_dir)\n",
    "            # adv = torch.load(image_dir)\n",
    "            # real_label = torch.load(os.path.join(real_dir, str(i) + '_label.pt'))\n",
    "            model.eval()\n",
    "            predicted_label = model(img).data.max(1, keepdim=True)[1][0]\n",
    "            real_label = label\n",
    "            if real_label == predicted_label: # TODO: edit\n",
    "                cout -= 1\n",
    "                continue\n",
    "            iters += 1\n",
    "            if iters > 5:\n",
    "                break\n",
    "            val = targeted_detection(model, img, label, dataset, targeted_lr, t_radius)\n",
    "            vals = np.concatenate((vals, [val]))\n",
    "            \n",
    "        print('this is the number of successes in targeted detection', cout)\n",
    "    return vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.015800952911376953,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Validating...",
       "rate": null,
       "total": 100000,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b8ce312d03c4eb59aef03a31ab07180",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating...:   0%|          | 0/100000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "real_dir = \"/Users/albertwen/Downloads/mae_data/ImageNet-Data/val\"\n",
    "adv_dir = \"/Users/albertwen/Downloads/mae_data/ImageNet-Data/attack/val\"\n",
    "\n",
    "values = targeted_vals(pretrained_model, \"imagenet\", \"real\", real_dir, adv_dir, targeted_lr=0.005, t_radius=0.03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([200.,  77., 118., 200., 200., 200.])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.011174917221069336,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Validating...",
       "rate": null,
       "total": 400284,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e857b8a07ba410bbd21bf3e54cc37bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating...:   0%|          | 0/400284 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is the number of successes in targeted detection -46\n"
     ]
    }
   ],
   "source": [
    "targeted_atk_vals = targeted_vals(pretrained_model, \"imagenet\", \"fgsm_4\", real_dir, adv_dir, targeted_lr=0.005, t_radius=0.03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targeted_atk_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m untargeted_values \u001b[39m=\u001b[39m untargeted_vals(model, val_dataset_loader, \u001b[39m\"\u001b[39;49m\u001b[39mreal\u001b[39;49m\u001b[39m\"\u001b[39;49m, real_dir, adv_dir, untargeted_lr\u001b[39m=\u001b[39;49m\u001b[39m0.1\u001b[39;49m, u_radius\u001b[39m=\u001b[39;49m\u001b[39m0.03\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[38], line 17\u001b[0m, in \u001b[0;36muntargeted_vals\u001b[0;34m(model, dataset, attack, real_dir, adv_dir, untargeted_lr, u_radius)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[39mfor\u001b[39;00m img, label \u001b[39min\u001b[39;00m (val_dataset_loader):\n\u001b[1;32m     10\u001b[0m \n\u001b[1;32m     11\u001b[0m     \u001b[39m# for i in range(lowind, upind):\u001b[39;00m\n\u001b[1;32m     12\u001b[0m         \u001b[39m# image_dir = os.path.join(real_dir, str(i) + '_img.pt')\u001b[39;00m\n\u001b[1;32m     13\u001b[0m         \u001b[39m# assert os.path.exists(image_dir)\u001b[39;00m\n\u001b[1;32m     14\u001b[0m         \u001b[39m# view_data = torch.load(image_dir)\u001b[39;00m\n\u001b[1;32m     16\u001b[0m         model\u001b[39m.\u001b[39meval()\n\u001b[0;32m---> 17\u001b[0m         val \u001b[39m=\u001b[39m untargeted_detection(model, img, label, \u001b[39m\"\u001b[39;49m\u001b[39mimagenet\u001b[39;49m\u001b[39m\"\u001b[39;49m, untargeted_lr, u_radius)\n\u001b[1;32m     18\u001b[0m         vals \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mconcatenate((vals, [val]))\n\u001b[1;32m     19\u001b[0m \u001b[39melse\u001b[39;00m: \n",
      "Cell \u001b[0;32mIn[35], line 20\u001b[0m, in \u001b[0;36muntargeted_detection\u001b[0;34m(model, img, true_label, dataset, lr, u_radius, cap, margin, use_margin)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[39melse\u001b[39;00m: \n\u001b[1;32m     18\u001b[0m     \u001b[39m# loss = -F.cross_entropy(output, torch.LongTensor([true_label]).cuda())\u001b[39;00m\n\u001b[1;32m     19\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39m-\u001b[39mF\u001b[39m.\u001b[39mcross_entropy(output, torch\u001b[39m.\u001b[39mLongTensor([true_label]))\n\u001b[0;32m---> 20\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     22\u001b[0m x_var\u001b[39m.\u001b[39mdata \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mclamp(x_var \u001b[39m-\u001b[39m lr \u001b[39m*\u001b[39m x_var\u001b[39m.\u001b[39mgrad\u001b[39m.\u001b[39mdata, \u001b[39mmin\u001b[39m\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, \u001b[39mmax\u001b[39m\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     23\u001b[0m x_var\u001b[39m.\u001b[39mdata \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mclamp(x_var \u001b[39m-\u001b[39m img, \u001b[39mmin\u001b[39m\u001b[39m=\u001b[39m\u001b[39m-\u001b[39mu_radius, \u001b[39mmax\u001b[39m\u001b[39m=\u001b[39mu_radius) \u001b[39m+\u001b[39m img\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ml/lib/python3.8/site-packages/torch/tensor.py:245\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    237\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    238\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    239\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    243\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[1;32m    244\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[0;32m--> 245\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ml/lib/python3.8/site-packages/torch/autograd/__init__.py:145\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[39mif\u001b[39;00m retain_graph \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    143\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m--> 145\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(\n\u001b[1;32m    146\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    147\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "untargeted_values = untargeted_vals(pretrained_model, \"imagenet\", \"real\", real_dir, adv_dir, untargeted_lr=0.1, u_radius=0.03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'untargeted_values' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m untargeted_values\n",
      "\u001b[0;31mNameError\u001b[0m: name 'untargeted_values' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f9992779686633b1a0c917c3f6317d760e57d78a77247aa4b25ad344de6939a7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
