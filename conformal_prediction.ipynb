{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import linalg\n",
    "from scipy.spatial.distance import pdist\n",
    "from scipy.spatial.distance import squareform\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors as sknn\n",
    "\n",
    "import pickle\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# push file to git repo\n",
    "# split clean sample and put into test set as negative classes (instead of current methodology)\n",
    "# cross validation on the clean sample is another idea?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/users/albertwen/downloads/mae_data/val_losses.pkl\", \"rb\") as f:\n",
    "  raw_stdout = pickle.load(f)\n",
    "split = raw_stdout.split('\\n')\n",
    "pattern = re.compile(\"Epoch: .*\")\n",
    "base_loss = [i for i in split if pattern.match(i)][:-1]\n",
    "base_loss = [float(re.search('loss: (0\\.\\d*) .*', i).group(1)) for i in base_loss]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/users/albertwen/downloads/mae_data/fgsm8_losses.pkl\", \"rb\") as f:\n",
    "  raw_stdout = pickle.load(f)\n",
    "split = raw_stdout.split('\\n')\n",
    "pattern = re.compile(\"Epoch: .*\")\n",
    "fgsm8_loss = [i for i in split if pattern.match(i)][:-1]\n",
    "fgsm8_loss = [float(re.search('loss: (0\\.\\d*) .*', i).group(1)) for i in fgsm8_loss]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conformal Predictor Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConformalAnomalyDetector():\n",
    "    \"\"\"\n",
    "    Conformal Anomaly Detector Class\n",
    "    @params\n",
    "        ICM: class\n",
    "            An object whose call operation should produce an array of conformal scores\n",
    "        z: tuple (len==2)\n",
    "            Each element is an (x,y) pair of the training set for CAD\n",
    "        significance: float\n",
    "            The significance level (must be between 0 and 1 exclusive)\n",
    "    \"\"\"\n",
    "    def __init__ (self, train, ICM, significance=0.05):\n",
    "        self._ICM = ICM\n",
    "        self.train = train\n",
    "\n",
    "        assert significance > 0 and significance < 1, \\\n",
    "            print('Significance must be in range (0,1).')\n",
    "        self._significance = significance\n",
    "\n",
    "        self.baseline = self._ICM(self.train, self.train)\n",
    "\n",
    "        \n",
    "    \"\"\"\n",
    "    Return true or false if the test example are an anomaly\n",
    "    @params\n",
    "        test: np.ndarray\n",
    "            A 1xn test example where m is the number of test examples and n is the number of dimensions\n",
    "    @return: bool\n",
    "        True if test input is anomaly and false otherwise \n",
    "    \"\"\"\n",
    "    def testIfAnomaly(self, test_val):\n",
    "        # conformal_set = np.concatenate((self.z,np.asarray(test)))\n",
    "        # conformal_set = np.asarray(self.z + [self.test_test]).reshape((len(self.z) + 1, 1))\n",
    "        conformal_scores = self._ICM(self.train, test_val)\n",
    "\n",
    "        p = np.sum(self.baseline >= conformal_scores[0]) / len(self.baseline)\n",
    "        # p = np.sum(conformal_scores >= conformal_scores[-1]) / (len(self.train)+1)\n",
    "        \n",
    "        return p < self._significance\n",
    "\n",
    "    \"\"\"\n",
    "    Return array of true or false if the test examples are an anomaly\n",
    "    @params\n",
    "        test: np.ndarray\n",
    "            A mxn test example where m is the number of test examples and n is the number of dimensions\n",
    "    @return: np.ndarray\n",
    "        An mx1 array of true if test input is anomaly and false otherwise \n",
    "    \"\"\" \n",
    "    def __call__(self,test_set):\n",
    "        isAnomaly = [self.testIfAnomaly(test_set[i]) for i in range(len(test_set))]\n",
    "        return isAnomaly\n",
    "\n",
    "    \"\"\"\n",
    "    Change significance level (hyper-parameter)\n",
    "    @params\n",
    "        significance: float\n",
    "            The significance level (must be between 0 and 1 exclusive)\n",
    "    \"\"\" \n",
    "    def set_significance(self,significance):\n",
    "        assert significance > 0 and significance < 1, \\\n",
    "            print('Significance must be in range (0,1).')\n",
    "        self._significance = significance"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNearestNeighbors():\n",
    "\n",
    "    # change this to return distance[0] from sklearn nearest neighbors implementation\n",
    "    # described below\n",
    "    \"\"\"\n",
    "        A simple real-valued function to compute the conformal scores\n",
    "        Each conformal score is the average k-nearest neighbors according to a specified metric\n",
    "        @params\n",
    "            k: int\n",
    "                Determines k nearest neighbors\n",
    "            metric: str\n",
    "                distance metric (see scipy's pdist function for valid metrics)\n",
    "    \"\"\"\n",
    "    def __init__(self, k, metric='euclidean'):\n",
    "        self._k = k\n",
    "        self._metric = metric\n",
    "        self.neighbors = sknn(n_neighbors=k)\n",
    "\n",
    "        # self.all_distances = pdist(np.asarray(train_set + test_set).reshape((len(train_set) + len(test_set), 1)), self._metric)\n",
    "        # self.all_distances = squareform(self.all_distances)\n",
    "\n",
    "        # self.train_distances = self.all_distances[:len(self.train)].T[:len(self.train)].T\n",
    "\n",
    "    \"\"\"\n",
    "        Returns a pairwise distance matrix\n",
    "        @params\n",
    "            x: np.ndarray\n",
    "                An m x n array with m samples and n dimensions\n",
    "    \"\"\"\n",
    "    def get_pairwise_distance_matrix(self, test_val):\n",
    "        distances = self.neighbors.kneighbors(test_val, self._k, return_distance=True)\n",
    "        return np.mean(distances[0], axis=1)\n",
    "\n",
    "    \"\"\"\n",
    "        Returns the mean pairwise distance between the k'th nearest neighbors\n",
    "        @params\n",
    "            x: np.ndarray\n",
    "                An m x n array with m samples and n dimensions\n",
    "    \"\"\"\n",
    "    def __call__(self, train, test_val):\n",
    "        self.neighbors.fit(np.asarray(train).reshape((len(train), 1)))\n",
    "\n",
    "        if np.array(test_val).shape == ():\n",
    "            distance_matrix = self.get_pairwise_distance_matrix(np.array([[test_val]]))\n",
    "        else: \n",
    "            distance_matrix = self.get_pairwise_distance_matrix(np.array(test_val).reshape((len(test_val), 1)))\n",
    "        return distance_matrix"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FGSM8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(base_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = base_loss[:50000]\n",
    "test_data = fgsm8_loss\n",
    "# test_data = base_loss[:100]\n",
    "\n",
    "np.random.seed(123432) # set seed for reproducibility\n",
    "k_nearest_neighbor = KNearestNeighbors(k=1000) # Initialize the ICM that uses k-nearest neighbors(k=10)\n",
    "conformal_predictor = ConformalAnomalyDetector(train_data, ICM=k_nearest_neighbor) # initialize CAD\n",
    "\n",
    "significances = [0.025,0.05,0.25,0.5] # see how different significance levels affect results\n",
    "# significances = [0.025]\n",
    "\n",
    "anomalies = []\n",
    "for i in range(len(significances)):\n",
    "    significance = significances[i]\n",
    "    conformal_predictor.set_significance(significance) # change significance\n",
    "    isAnomaly = conformal_predictor(test_data) # test if anomamlies according to current CAD\n",
    "    anomalies.append(isAnomaly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/users/albertwen/downloads/mae_data/fgsm4_anomalies.pkl\", \"wb\") as f:\n",
    "    pickle.dump(anomalies, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = base_loss[:50000]\n",
    "# test_data = fgsm8_loss\n",
    "test_data = base_loss[50000:]\n",
    "\n",
    "# improve performance using vstack and cupy instead of numpy\n",
    "\n",
    "np.random.seed(123432) # set seed for reproducibility\n",
    "k_nearest_neighbor = KNearestNeighbors(k=1000) # Initialize the ICM that uses k-nearest neighbors(k=10)\n",
    "conformal_predictor = ConformalAnomalyDetector(train_data, ICM=k_nearest_neighbor) # initialize CAD\n",
    "\n",
    "significances = [0.025,0.05,0.25,0.5] # see how different significance levels affect results\n",
    "# significances = [0.025]\n",
    "\n",
    "base_anomalies = []\n",
    "for i in range(len(significances)):\n",
    "    significance = significances[i]\n",
    "    conformal_predictor.set_significance(significance) # change significance\n",
    "    isAnomaly = conformal_predictor(test_data) # test if anomamlies according to current CAD\n",
    "    base_anomalies.append(isAnomaly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/users/albertwen/downloads/mae_data/baseline_anomalies.pkl\", \"wb\") as f:\n",
    "    pickle.dump(base_anomalies, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tpr 0.48446\n",
      "tpr 0.62556\n",
      "tpr 0.9149\n",
      "tpr 0.97602\n"
     ]
    }
   ],
   "source": [
    "for result in anomalies:\n",
    "    print(\"tpr\", sum(result) / len(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fpr 0.02796\n",
      "fpr 0.05556\n",
      "fpr 0.26408\n",
      "fpr 0.51588\n"
     ]
    }
   ],
   "source": [
    "for result in base_anomalies:\n",
    "    print(\"fpr\", sum(result) / len(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.72825\n",
      "accuracy 0.785\n",
      "accuracy 0.82541\n",
      "accuracy 0.73007\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(anomalies)):\n",
    "    print(\"accuracy\", (sum(anomalies[i]) + (len(base_anomalies[i]) - sum(base_anomalies[i]))) / (len(base_anomalies[i]) + len(anomalies[i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24206"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(base_anomalies[3]) - sum(base_anomalies[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(base_anomalies[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(base_anomalies[3]) + len(anomalies[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48801"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(anomalies[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6590266666666666"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(50079 + 48775) / 150000"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## testing with different k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = base_loss[:50000]\n",
    "test_data = fgsm8_loss\n",
    "# test_data = base_loss[50000:]\n",
    "\n",
    "np.random.seed(123432) # set seed for reproducibility\n",
    "k_nearest_neighbor = KNearestNeighbors(k=100) # Initialize the ICM that uses k-nearest neighbors(k=10)\n",
    "conformal_predictor = ConformalAnomalyDetector(train_data, ICM=k_nearest_neighbor) # initialize CAD\n",
    "\n",
    "significances = [0.025,0.05,0.25,0.5] # see how different significance levels affect results\n",
    "# significances = [0.025]\n",
    "\n",
    "anomalies = []\n",
    "for i in range(len(significances)):\n",
    "    significance = significances[i]\n",
    "    conformal_predictor.set_significance(significance) # change significance\n",
    "    isAnomaly = conformal_predictor(test_data) # test if anomamlies according to current CAD\n",
    "    anomalies.append(isAnomaly)\n",
    "\n",
    "with open(\"/users/albertwen/downloads/mae_data/fgsm8_anomalies_k100.pkl\", \"wb\") as f:\n",
    "    pickle.dump(anomalies, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = base_loss[:50000]\n",
    "# test_data = fgsm8_loss\n",
    "test_data = base_loss[50000:]\n",
    "\n",
    "np.random.seed(123432) # set seed for reproducibility\n",
    "k_nearest_neighbor = KNearestNeighbors(k=100) # Initialize the ICM that uses k-nearest neighbors(k=10)\n",
    "conformal_predictor = ConformalAnomalyDetector(train_data, ICM=k_nearest_neighbor) # initialize CAD\n",
    "\n",
    "significances = [0.025,0.05,0.25,0.5] # see how different significance levels affect results\n",
    "# significances = [0.025]\n",
    "\n",
    "anomalies = []\n",
    "for i in range(len(significances)):\n",
    "    significance = significances[i]\n",
    "    conformal_predictor.set_significance(significance) # change significance\n",
    "    isAnomaly = conformal_predictor(test_data) # test if anomamlies according to current CAD\n",
    "    anomalies.append(isAnomaly)\n",
    "\n",
    "with open(\"/users/albertwen/downloads/mae_data/base_anomalies_k100.pkl\", \"wb\") as f:\n",
    "    pickle.dump(anomalies, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tpr 0.03\n",
      "tpr 0.06242\n",
      "tpr 0.31272\n",
      "tpr 0.58346\n",
      "fpr 0.02378\n",
      "fpr 0.0477\n",
      "fpr 0.24348\n",
      "fpr 0.49394\n",
      "accuracy 0.50311\n",
      "accuracy 0.50736\n",
      "accuracy 0.53462\n",
      "accuracy 0.54476\n"
     ]
    }
   ],
   "source": [
    "for result in anomalies:\n",
    "    print(\"tpr\", sum(result) / len(result))\n",
    "for result in base_anomalies:\n",
    "    print(\"fpr\", sum(result) / len(result))\n",
    "for i in range(len(anomalies)):\n",
    "    print(\"accuracy\", (sum(anomalies[i]) + (len(base_anomalies[i]) - sum(base_anomalies[i]))) / (len(base_anomalies[i]) + len(anomalies[i])))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## testing with k=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = base_loss[:50000]\n",
    "test_data = fgsm8_loss\n",
    "# test_data = base_loss[50000:]\n",
    "\n",
    "np.random.seed(123432) # set seed for reproducibility\n",
    "k_nearest_neighbor = KNearestNeighbors(k=10) # Initialize the ICM that uses k-nearest neighbors(k=10)\n",
    "conformal_predictor = ConformalAnomalyDetector(train_data, ICM=k_nearest_neighbor) # initialize CAD\n",
    "\n",
    "significances = [0.025,0.05,0.25,0.5] # see how different significance levels affect results\n",
    "# significances = [0.025]\n",
    "\n",
    "anomalies = []\n",
    "for i in range(len(significances)):\n",
    "    significance = significances[i]\n",
    "    conformal_predictor.set_significance(significance) # change significance\n",
    "    isAnomaly = conformal_predictor(test_data) # test if anomamlies according to current CAD\n",
    "    anomalies.append(isAnomaly)\n",
    "\n",
    "with open(\"/users/albertwen/downloads/mae_data/fgsm8_anomalies_k10.pkl\", \"wb\") as f:\n",
    "    pickle.dump(anomalies, f)\n",
    "\n",
    "train_data = base_loss[:50000]\n",
    "# test_data = fgsm8_loss\n",
    "test_data = base_loss[50000:]\n",
    "\n",
    "# improve performance using vstack and cupy instead of numpy\n",
    "\n",
    "np.random.seed(123432) # set seed for reproducibility\n",
    "k_nearest_neighbor = KNearestNeighbors(k=10) # Initialize the ICM that uses k-nearest neighbors(k=10)\n",
    "conformal_predictor = ConformalAnomalyDetector(train_data, ICM=k_nearest_neighbor) # initialize CAD\n",
    "\n",
    "significances = [0.025,0.05,0.25,0.5] # see how different significance levels affect results\n",
    "# significances = [0.025]\n",
    "\n",
    "anomalies = []\n",
    "for i in range(len(significances)):\n",
    "    significance = significances[i]\n",
    "    conformal_predictor.set_significance(significance) # change significance\n",
    "    isAnomaly = conformal_predictor(test_data) # test if anomamlies according to current CAD\n",
    "    anomalies.append(isAnomaly)\n",
    "\n",
    "with open(\"/users/albertwen/downloads/mae_data/base_anomalies_k10.pkl\", \"wb\") as f:\n",
    "    pickle.dump(anomalies, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tpr 0.05602\n",
      "tpr 0.0852\n",
      "tpr 0.0852\n",
      "tpr 0.0852\n",
      "fpr 0.02378\n",
      "fpr 0.0477\n",
      "fpr 0.24348\n",
      "fpr 0.49394\n",
      "accuracy 0.51612\n",
      "accuracy 0.51875\n",
      "accuracy 0.42086\n",
      "accuracy 0.29563\n"
     ]
    }
   ],
   "source": [
    "for result in anomalies:\n",
    "    print(\"tpr\", sum(result) / len(result))\n",
    "for result in base_anomalies:\n",
    "    print(\"fpr\", sum(result) / len(result))\n",
    "for i in range(len(anomalies)):\n",
    "    print(\"accuracy\", (sum(anomalies[i]) + (len(base_anomalies[i]) - sum(base_anomalies[i]))) / (len(base_anomalies[i]) + len(anomalies[i])))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FGSM4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/users/albertwen/downloads/mae_data/fgsm4_losses.pkl\", \"rb\") as f:\n",
    "  raw_stdout = pickle.load(f)\n",
    "split = raw_stdout.split('\\n')\n",
    "pattern = re.compile(\"Epoch: .*\")\n",
    "fgsm4_loss = [i for i in split if pattern.match(i)][:-1]\n",
    "fgsm4_loss = [float(re.search('loss: (0\\.\\d*) .*', i).group(1)) for i in fgsm4_loss]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = base_loss[:50000]\n",
    "test_data = fgsm4_loss\n",
    "# test_data = base_loss[:100]\n",
    "\n",
    "np.random.seed(123432) # set seed for reproducibility\n",
    "k_nearest_neighbor = KNearestNeighbors(k=1000) # Initialize the ICM that uses k-nearest neighbors(k=10)\n",
    "conformal_predictor = ConformalAnomalyDetector(train_data, ICM=k_nearest_neighbor) # initialize CAD\n",
    "\n",
    "significances = [0.025,0.05,0.25,0.5] # see how different significance levels affect results\n",
    "# significances = [0.025]\n",
    "\n",
    "anomalies = []\n",
    "for i in range(len(significances)):\n",
    "    significance = significances[i]\n",
    "    conformal_predictor.set_significance(significance) # change significance\n",
    "    isAnomaly = conformal_predictor(test_data) # test if anomamlies according to current CAD\n",
    "    anomalies.append(isAnomaly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/users/albertwen/downloads/mae_data/fgsm4_anomalies.pkl\", \"wb\") as f:\n",
    "    pickle.dump(anomalies, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data = base_loss[:50000]\n",
    "# # test_data = fgsm8_loss\n",
    "# test_data = base_loss[50000:]\n",
    "\n",
    "# # improve performance using vstack and cupy instead of numpy\n",
    "\n",
    "# np.random.seed(123432) # set seed for reproducibility\n",
    "# k_nearest_neighbor = KNearestNeighbors(k=1000) # Initialize the ICM that uses k-nearest neighbors(k=10)\n",
    "# conformal_predictor = ConformalAnomalyDetector(train_data, ICM=k_nearest_neighbor) # initialize CAD\n",
    "\n",
    "# significances = [0.025,0.05,0.25,0.5] # see how different significance levels affect results\n",
    "# # significances = [0.025]\n",
    "\n",
    "# base_anomalies = []\n",
    "# for i in range(len(significances)):\n",
    "#     significance = significances[i]\n",
    "#     conformal_predictor.set_significance(significance) # change significance\n",
    "#     isAnomaly = conformal_predictor(test_data) # test if anomamlies according to current CAD\n",
    "#     base_anomalies.append(isAnomaly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/users/albertwen/downloads/mae_data/baseline_anomalies.pkl\", \"rb\") as f:\n",
    "    base_anomalies = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tpr 0.10346\n",
      "tpr 0.17068\n",
      "tpr 0.48784\n",
      "tpr 0.7087\n"
     ]
    }
   ],
   "source": [
    "for result in anomalies:\n",
    "    print(\"tpr\", sum(result) / len(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fpr 0.02796\n",
      "fpr 0.05556\n",
      "fpr 0.26408\n",
      "fpr 0.51588\n"
     ]
    }
   ],
   "source": [
    "for result in base_anomalies:\n",
    "    print(\"fpr\", sum(result) / len(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.53775\n",
      "accuracy 0.55756\n",
      "accuracy 0.61188\n",
      "accuracy 0.59641\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(anomalies)):\n",
    "    print(\"accuracy\", (sum(anomalies[i]) + (len(base_anomalies[i]) - sum(base_anomalies[i]))) / (len(base_anomalies[i]) + len(anomalies[i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24206"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(base_anomalies[3]) - sum(base_anomalies[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(base_anomalies[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(base_anomalies[3]) + len(anomalies[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35435"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(anomalies[3])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PGDL2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/users/albertwen/downloads/mae_data/pgdl2_losses.pkl\", \"rb\") as f:\n",
    "  raw_stdout = pickle.load(f)\n",
    "split = raw_stdout.split('\\n')\n",
    "pattern = re.compile(\"Epoch: .*\")\n",
    "pgdl2_loss = [i for i in split if pattern.match(i)][:-1]\n",
    "pgdl2_loss = [float(re.search('loss: (0\\.\\d*) .*', i).group(1)) for i in pgdl2_loss]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = base_loss[:50000]\n",
    "test_data = pgdl2_loss\n",
    "# test_data = base_loss[:100]\n",
    "\n",
    "np.random.seed(123432) # set seed for reproducibility\n",
    "k_nearest_neighbor = KNearestNeighbors(k=1000) # Initialize the ICM that uses k-nearest neighbors(k=10)\n",
    "conformal_predictor = ConformalAnomalyDetector(train_data, ICM=k_nearest_neighbor) # initialize CAD\n",
    "\n",
    "significances = [0.025,0.05,0.25,0.5] # see how different significance levels affect results\n",
    "# significances = [0.025]\n",
    "\n",
    "anomalies = []\n",
    "for i in range(len(significances)):\n",
    "    significance = significances[i]\n",
    "    conformal_predictor.set_significance(significance) # change significance\n",
    "    isAnomaly = conformal_predictor(test_data) # test if anomamlies according to current CAD\n",
    "    anomalies.append(isAnomaly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/users/albertwen/downloads/mae_data/pgdl2_anomalies.pkl\", \"wb\") as f:\n",
    "    pickle.dump(anomalies, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data = base_loss[:50000]\n",
    "# # test_data = fgsm8_loss\n",
    "# test_data = base_loss[50000:]\n",
    "\n",
    "# # improve performance using vstack and cupy instead of numpy\n",
    "\n",
    "# np.random.seed(123432) # set seed for reproducibility\n",
    "# k_nearest_neighbor = KNearestNeighbors(k=1000) # Initialize the ICM that uses k-nearest neighbors(k=10)\n",
    "# conformal_predictor = ConformalAnomalyDetector(train_data, ICM=k_nearest_neighbor) # initialize CAD\n",
    "\n",
    "# significances = [0.025,0.05,0.25,0.5] # see how different significance levels affect results\n",
    "# # significances = [0.025]\n",
    "\n",
    "# base_anomalies = []\n",
    "# for i in range(len(significances)):\n",
    "#     significance = significances[i]\n",
    "#     conformal_predictor.set_significance(significance) # change significance\n",
    "#     isAnomaly = conformal_predictor(test_data) # test if anomamlies according to current CAD\n",
    "#     base_anomalies.append(isAnomaly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/users/albertwen/downloads/mae_data/baseline_anomalies.pkl\", \"rb\") as f:\n",
    "    base_anomalies = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tpr 0.03132058932605104\n",
      "tpr 0.055392603081487475\n",
      "tpr 0.2634772934155962\n",
      "tpr 0.5165368665924952\n"
     ]
    }
   ],
   "source": [
    "for result in anomalies:\n",
    "    print(\"tpr\", sum(result) / len(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fpr 0.02796\n",
      "fpr 0.05556\n",
      "fpr 0.26408\n",
      "fpr 0.51588\n"
     ]
    }
   ],
   "source": [
    "for result in base_anomalies:\n",
    "    print(\"fpr\", sum(result) / len(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.6217722599515447\n",
      "accuracy 0.6134118326408154\n",
      "accuracy 0.5600105446830946\n",
      "accuracy 0.49619010557236287\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(anomalies)):\n",
    "    print(\"accuracy\", (sum(anomalies[i]) + (len(base_anomalies[i]) - sum(base_anomalies[i]))) / (len(base_anomalies[i]) + len(anomalies[i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24206"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(base_anomalies[3]) - sum(base_anomalies[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(base_anomalies[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79661"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(base_anomalies[3]) + len(anomalies[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15321"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(anomalies[3])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# APGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[68], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m pattern \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39mcompile(\u001b[39m\"\u001b[39m\u001b[39mEpoch: .*\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m apgd_loss \u001b[39m=\u001b[39m [i \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m split \u001b[39mif\u001b[39;00m pattern\u001b[39m.\u001b[39mmatch(i)][:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[0;32m----> 6\u001b[0m apgd_loss \u001b[39m=\u001b[39m [\u001b[39mfloat\u001b[39m(re\u001b[39m.\u001b[39msearch(\u001b[39m'\u001b[39m\u001b[39mloss: (0\u001b[39m\u001b[39m\\\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\\\u001b[39m\u001b[39md*) .*\u001b[39m\u001b[39m'\u001b[39m, i)\u001b[39m.\u001b[39mgroup(\u001b[39m1\u001b[39m)) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m pgdl2_loss]\n",
      "Cell \u001b[0;32mIn[68], line 6\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      4\u001b[0m pattern \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39mcompile(\u001b[39m\"\u001b[39m\u001b[39mEpoch: .*\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m apgd_loss \u001b[39m=\u001b[39m [i \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m split \u001b[39mif\u001b[39;00m pattern\u001b[39m.\u001b[39mmatch(i)][:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[0;32m----> 6\u001b[0m apgd_loss \u001b[39m=\u001b[39m [\u001b[39mfloat\u001b[39m(re\u001b[39m.\u001b[39;49msearch(\u001b[39m'\u001b[39;49m\u001b[39mloss: (0\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39md*) .*\u001b[39;49m\u001b[39m'\u001b[39;49m, i)\u001b[39m.\u001b[39mgroup(\u001b[39m1\u001b[39m)) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m pgdl2_loss]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ml/lib/python3.8/re.py:201\u001b[0m, in \u001b[0;36msearch\u001b[0;34m(pattern, string, flags)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msearch\u001b[39m(pattern, string, flags\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m):\n\u001b[1;32m    199\u001b[0m     \u001b[39m\"\"\"Scan through string looking for a match to the pattern, returning\u001b[39;00m\n\u001b[1;32m    200\u001b[0m \u001b[39m    a Match object, or None if no match was found.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 201\u001b[0m     \u001b[39mreturn\u001b[39;00m _compile(pattern, flags)\u001b[39m.\u001b[39;49msearch(string)\n",
      "\u001b[0;31mTypeError\u001b[0m: expected string or bytes-like object"
     ]
    }
   ],
   "source": [
    "with open(\"/users/albertwen/downloads/mae_data/apgd_losses.pkl\", \"rb\") as f:\n",
    "  raw_stdout = pickle.load(f)\n",
    "split = raw_stdout.split('\\n')\n",
    "pattern = re.compile(\"Epoch: .*\")\n",
    "apgd_loss = [i for i in split if pattern.match(i)][:-1]\n",
    "apgd_loss = [float(re.search('loss: (0\\.\\d*) .*', i).group(1)) for i in pgdl2_loss]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = base_loss[:50000]\n",
    "test_data = apgd_loss\n",
    "# test_data = base_loss[:100]\n",
    "\n",
    "np.random.seed(123432) # set seed for reproducibility\n",
    "k_nearest_neighbor = KNearestNeighbors(k=1000) # Initialize the ICM that uses k-nearest neighbors(k=10)\n",
    "conformal_predictor = ConformalAnomalyDetector(train_data, ICM=k_nearest_neighbor) # initialize CAD\n",
    "\n",
    "significances = [0.025,0.05,0.25,0.5] # see how different significance levels affect results\n",
    "# significances = [0.025]\n",
    "\n",
    "anomalies = []\n",
    "for i in range(len(significances)):\n",
    "    significance = significances[i]\n",
    "    conformal_predictor.set_significance(significance) # change significance\n",
    "    isAnomaly = conformal_predictor(test_data) # test if anomamlies according to current CAD\n",
    "    anomalies.append(isAnomaly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/users/albertwen/downloads/mae_data/pgdl2_anomalies.pkl\", \"wb\") as f:\n",
    "    pickle.dump(anomalies, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data = base_loss[:50000]\n",
    "# # test_data = fgsm8_loss\n",
    "# test_data = base_loss[50000:]\n",
    "\n",
    "# # improve performance using vstack and cupy instead of numpy\n",
    "\n",
    "# np.random.seed(123432) # set seed for reproducibility\n",
    "# k_nearest_neighbor = KNearestNeighbors(k=1000) # Initialize the ICM that uses k-nearest neighbors(k=10)\n",
    "# conformal_predictor = ConformalAnomalyDetector(train_data, ICM=k_nearest_neighbor) # initialize CAD\n",
    "\n",
    "# significances = [0.025,0.05,0.25,0.5] # see how different significance levels affect results\n",
    "# # significances = [0.025]\n",
    "\n",
    "# base_anomalies = []\n",
    "# for i in range(len(significances)):\n",
    "#     significance = significances[i]\n",
    "#     conformal_predictor.set_significance(significance) # change significance\n",
    "#     isAnomaly = conformal_predictor(test_data) # test if anomamlies according to current CAD\n",
    "#     base_anomalies.append(isAnomaly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/users/albertwen/downloads/mae_data/baseline_anomalies.pkl\", \"rb\") as f:\n",
    "    base_anomalies = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tpr 0.03132058932605104\n",
      "tpr 0.055392603081487475\n",
      "tpr 0.2634772934155962\n",
      "tpr 0.5165368665924952\n"
     ]
    }
   ],
   "source": [
    "for result in anomalies:\n",
    "    print(\"tpr\", sum(result) / len(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fpr 0.02796\n",
      "fpr 0.05556\n",
      "fpr 0.26408\n",
      "fpr 0.51588\n"
     ]
    }
   ],
   "source": [
    "for result in base_anomalies:\n",
    "    print(\"fpr\", sum(result) / len(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.6217722599515447\n",
      "accuracy 0.6134118326408154\n",
      "accuracy 0.5600105446830946\n",
      "accuracy 0.49619010557236287\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(anomalies)):\n",
    "    print(\"accuracy\", (sum(anomalies[i]) + (len(base_anomalies[i]) - sum(base_anomalies[i]))) / (len(base_anomalies[i]) + len(anomalies[i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24206"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "len(base_anomalies[3]) - sum(base_anomalies[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "len(base_anomalies[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79661"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "len(base_anomalies[3]) + len(anomalies[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15321"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sum(anomalies[3])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.5 (default, Sep  4 2020, 02:22:02) \n[Clang 10.0.0 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f9992779686633b1a0c917c3f6317d760e57d78a77247aa4b25ad344de6939a7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
